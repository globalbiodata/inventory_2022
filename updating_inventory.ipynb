{"cells":[{"cell_type":"markdown","metadata":{"id":"x4whPVjZZa7x"},"source":["# Updating the Inventory\n","---\n","This notebook will provide the code necessary to perform an update to the Biodata Resource Inventory, using trained models.\n","\n","The steps include:\n","* Run new query on EuropePMC\n","* Classify new articles\n","* Run NER to get resource names for predicted positives\n","* Get URLs for predicted positives\n","* Gather other metadata\n","\n","\n","\n","# Setup\n","---\n","### Mount Drive\n","\n","First, mount Google Drive to have access to files necessary for the run:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1702,"status":"ok","timestamp":1675109206770,"user":{"displayName":"Kenneth Schackart","userId":"14619721059788161882"},"user_tz":420},"id":"BmwESzXcjXTb","outputId":"c938f5b9-9033-4ebd-c8a4-d7cc008390f0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/MyDrive/GitHub/inventory_2022\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","%cd /content/drive/MyDrive/GitHub/inventory_2022/"]},{"cell_type":"markdown","metadata":{"id":"6a7pMnIVbKXE"},"source":["Run the make target to install Python dependencies.\n","\n","You may see the error: `ERROR: pip's dependency resolver does not currently take into account all the packages that are installed`, but the code should run regardless.\n","\n"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"iBMUW3C0YIz4"},"outputs":[{"name":"stdout","output_type":"stream","text":["pip install -r requirements.txt\n","Requirement already satisfied: datasets==1.18.3 in ./venv38/lib/python3.8/site-packages (from -r requirements.txt (line 1)) (1.18.3)\n","Requirement already satisfied: kaleido==0.2.1 in ./venv38/lib/python3.8/site-packages (from -r requirements.txt (line 2)) (0.2.1)\n","Requirement already satisfied: nltk==3.6.1 in ./venv38/lib/python3.8/site-packages (from -r requirements.txt (line 3)) (3.6.1)\n","Collecting numpy==1.19 (from -r requirements.txt (line 4))\n","  Using cached numpy-1.19.0-cp38-cp38-manylinux2010_x86_64.whl (14.6 MB)\n","Requirement already satisfied: pandas==1.3.5 in ./venv38/lib/python3.8/site-packages (from -r requirements.txt (line 5)) (1.3.5)\n","Requirement already satisfied: plotly==5.1.0 in ./venv38/lib/python3.8/site-packages (from -r requirements.txt (line 6)) (5.1.0)\n","Requirement already satisfied: pyyaml in ./venv38/lib/python3.8/site-packages (from -r requirements.txt (line 7)) (6.0.1)\n","Requirement already satisfied: scikit-learn==0.24.1 in ./venv38/lib/python3.8/site-packages (from -r requirements.txt (line 8)) (0.24.1)\n","Requirement already satisfied: seqeval==1.2.2 in ./venv38/lib/python3.8/site-packages (from -r requirements.txt (line 9)) (1.2.2)\n","Requirement already satisfied: pulp<2.8 in ./venv38/lib/python3.8/site-packages (from -r requirements.txt (line 10)) (2.7.0)\n","Requirement already satisfied: snakemake==7.1.1 in ./venv38/lib/python3.8/site-packages (from -r requirements.txt (line 11)) (7.1.1)\n","Requirement already satisfied: tabulate==0.8 in ./venv38/lib/python3.8/site-packages (from -r requirements.txt (line 12)) (0.8.0)\n","Requirement already satisfied: torch==1.9.0 in ./venv38/lib/python3.8/site-packages (from -r requirements.txt (line 13)) (1.9.0)\n","Requirement already satisfied: transformers==4.16.2 in ./venv38/lib/python3.8/site-packages (from -r requirements.txt (line 14)) (4.16.2)\n","Requirement already satisfied: tqdm==4.63.0 in ./venv38/lib/python3.8/site-packages (from -r requirements.txt (line 15)) (4.63.0)\n","Requirement already satisfied: pycountry==22.3.5 in ./venv38/lib/python3.8/site-packages (from -r requirements.txt (line 16)) (22.3.5)\n","Requirement already satisfied: pytest==6.2.4 in ./venv38/lib/python3.8/site-packages (from -r requirements.txt (line 17)) (6.2.4)\n","Requirement already satisfied: flake8==3.9.2 in ./venv38/lib/python3.8/site-packages (from -r requirements.txt (line 18)) (3.9.2)\n","Requirement already satisfied: pylint==2.8.2 in ./venv38/lib/python3.8/site-packages (from -r requirements.txt (line 19)) (2.8.2)\n","Requirement already satisfied: mypy==0.812 in ./venv38/lib/python3.8/site-packages (from -r requirements.txt (line 20)) (0.812)\n","Requirement already satisfied: pytest-flake8==1.0.7 in ./venv38/lib/python3.8/site-packages (from -r requirements.txt (line 21)) (1.0.7)\n","Requirement already satisfied: pytest-pylint==0.18.0 in ./venv38/lib/python3.8/site-packages (from -r requirements.txt (line 22)) (0.18.0)\n","Requirement already satisfied: pytest-mypy==0.8.1 in ./venv38/lib/python3.8/site-packages (from -r requirements.txt (line 23)) (0.8.1)\n","Requirement already satisfied: requests==2.27.1 in ./venv38/lib/python3.8/site-packages (from -r requirements.txt (line 24)) (2.27.1)\n","Requirement already satisfied: urllib3==1.26.8 in ./venv38/lib/python3.8/site-packages (from -r requirements.txt (line 25)) (1.26.8)\n","Requirement already satisfied: pyarrow!=4.0.0,>=3.0.0 in ./venv38/lib/python3.8/site-packages (from datasets==1.18.3->-r requirements.txt (line 1)) (15.0.0)\n","Requirement already satisfied: dill in ./venv38/lib/python3.8/site-packages (from datasets==1.18.3->-r requirements.txt (line 1)) (0.3.8)\n","Requirement already satisfied: xxhash in ./venv38/lib/python3.8/site-packages (from datasets==1.18.3->-r requirements.txt (line 1)) (3.4.1)\n","Requirement already satisfied: multiprocess in ./venv38/lib/python3.8/site-packages (from datasets==1.18.3->-r requirements.txt (line 1)) (0.70.15)\n","Requirement already satisfied: fsspec>=2021.05.0 in ./venv38/lib/python3.8/site-packages (from fsspec[http]>=2021.05.0->datasets==1.18.3->-r requirements.txt (line 1)) (2023.12.2)\n","Requirement already satisfied: aiohttp in ./venv38/lib/python3.8/site-packages (from datasets==1.18.3->-r requirements.txt (line 1)) (3.9.1)\n","Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in ./venv38/lib/python3.8/site-packages (from datasets==1.18.3->-r requirements.txt (line 1)) (0.20.3)\n","Requirement already satisfied: packaging in ./venv38/lib/python3.8/site-packages (from datasets==1.18.3->-r requirements.txt (line 1)) (23.2)\n","Requirement already satisfied: click in ./venv38/lib/python3.8/site-packages (from nltk==3.6.1->-r requirements.txt (line 3)) (8.1.7)\n","Requirement already satisfied: joblib in ./venv38/lib/python3.8/site-packages (from nltk==3.6.1->-r requirements.txt (line 3)) (1.3.2)\n","Requirement already satisfied: regex in ./venv38/lib/python3.8/site-packages (from nltk==3.6.1->-r requirements.txt (line 3)) (2023.12.25)\n","Requirement already satisfied: python-dateutil>=2.7.3 in ./venv38/lib/python3.8/site-packages (from pandas==1.3.5->-r requirements.txt (line 5)) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in ./venv38/lib/python3.8/site-packages (from pandas==1.3.5->-r requirements.txt (line 5)) (2023.3.post1)\n","Requirement already satisfied: tenacity>=6.2.0 in ./venv38/lib/python3.8/site-packages (from plotly==5.1.0->-r requirements.txt (line 6)) (8.2.3)\n","Requirement already satisfied: six in ./venv38/lib/python3.8/site-packages (from plotly==5.1.0->-r requirements.txt (line 6)) (1.16.0)\n","Requirement already satisfied: scipy>=0.19.1 in ./venv38/lib/python3.8/site-packages (from scikit-learn==0.24.1->-r requirements.txt (line 8)) (1.9.3)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in ./venv38/lib/python3.8/site-packages (from scikit-learn==0.24.1->-r requirements.txt (line 8)) (3.2.0)\n","Requirement already satisfied: wrapt in ./venv38/lib/python3.8/site-packages (from snakemake==7.1.1->-r requirements.txt (line 11)) (1.12.1)\n","Requirement already satisfied: ratelimiter in ./venv38/lib/python3.8/site-packages (from snakemake==7.1.1->-r requirements.txt (line 11)) (1.2.0.post0)\n","Requirement already satisfied: configargparse in ./venv38/lib/python3.8/site-packages (from snakemake==7.1.1->-r requirements.txt (line 11)) (1.7)\n","Requirement already satisfied: appdirs in ./venv38/lib/python3.8/site-packages (from snakemake==7.1.1->-r requirements.txt (line 11)) (1.4.4)\n","Requirement already satisfied: datrie in ./venv38/lib/python3.8/site-packages (from snakemake==7.1.1->-r requirements.txt (line 11)) (0.8.2)\n","Requirement already satisfied: jsonschema in ./venv38/lib/python3.8/site-packages (from snakemake==7.1.1->-r requirements.txt (line 11)) (4.21.1)\n","Requirement already satisfied: docutils in ./venv38/lib/python3.8/site-packages (from snakemake==7.1.1->-r requirements.txt (line 11)) (0.20.1)\n","Requirement already satisfied: gitpython in ./venv38/lib/python3.8/site-packages (from snakemake==7.1.1->-r requirements.txt (line 11)) (3.1.41)\n","Requirement already satisfied: psutil in ./venv38/lib/python3.8/site-packages (from snakemake==7.1.1->-r requirements.txt (line 11)) (5.9.8)\n","Requirement already satisfied: nbformat in ./venv38/lib/python3.8/site-packages (from snakemake==7.1.1->-r requirements.txt (line 11)) (5.9.2)\n","Requirement already satisfied: toposort in ./venv38/lib/python3.8/site-packages (from snakemake==7.1.1->-r requirements.txt (line 11)) (1.10)\n","Requirement already satisfied: connection-pool>=0.0.3 in ./venv38/lib/python3.8/site-packages (from snakemake==7.1.1->-r requirements.txt (line 11)) (0.0.3)\n","Requirement already satisfied: smart-open>=3.0 in ./venv38/lib/python3.8/site-packages (from snakemake==7.1.1->-r requirements.txt (line 11)) (6.4.0)\n","Requirement already satisfied: filelock in ./venv38/lib/python3.8/site-packages (from snakemake==7.1.1->-r requirements.txt (line 11)) (3.13.1)\n","Requirement already satisfied: stopit in ./venv38/lib/python3.8/site-packages (from snakemake==7.1.1->-r requirements.txt (line 11)) (1.1.2)\n","Requirement already satisfied: yte<2.0,>=1.0 in ./venv38/lib/python3.8/site-packages (from snakemake==7.1.1->-r requirements.txt (line 11)) (1.5.4)\n","Requirement already satisfied: jinja2<4.0,>=3.0 in ./venv38/lib/python3.8/site-packages (from snakemake==7.1.1->-r requirements.txt (line 11)) (3.1.3)\n","Requirement already satisfied: retry in ./venv38/lib/python3.8/site-packages (from snakemake==7.1.1->-r requirements.txt (line 11)) (0.9.2)\n","Requirement already satisfied: typing-extensions in ./venv38/lib/python3.8/site-packages (from torch==1.9.0->-r requirements.txt (line 13)) (4.9.0)\n","Requirement already satisfied: sacremoses in ./venv38/lib/python3.8/site-packages (from transformers==4.16.2->-r requirements.txt (line 14)) (0.1.1)\n","Requirement already satisfied: tokenizers!=0.11.3,>=0.10.1 in ./venv38/lib/python3.8/site-packages (from transformers==4.16.2->-r requirements.txt (line 14)) (0.15.1)\n","Requirement already satisfied: setuptools in ./venv38/lib/python3.8/site-packages (from pycountry==22.3.5->-r requirements.txt (line 16)) (69.0.2)\n","Requirement already satisfied: attrs>=19.2.0 in ./venv38/lib/python3.8/site-packages (from pytest==6.2.4->-r requirements.txt (line 17)) (23.2.0)\n","Requirement already satisfied: iniconfig in ./venv38/lib/python3.8/site-packages (from pytest==6.2.4->-r requirements.txt (line 17)) (2.0.0)\n","Requirement already satisfied: pluggy<1.0.0a1,>=0.12 in ./venv38/lib/python3.8/site-packages (from pytest==6.2.4->-r requirements.txt (line 17)) (0.13.1)\n","Requirement already satisfied: py>=1.8.2 in ./venv38/lib/python3.8/site-packages (from pytest==6.2.4->-r requirements.txt (line 17)) (1.11.0)\n","Requirement already satisfied: toml in ./venv38/lib/python3.8/site-packages (from pytest==6.2.4->-r requirements.txt (line 17)) (0.10.2)\n","Requirement already satisfied: pyflakes<2.4.0,>=2.3.0 in ./venv38/lib/python3.8/site-packages (from flake8==3.9.2->-r requirements.txt (line 18)) (2.3.1)\n","Requirement already satisfied: pycodestyle<2.8.0,>=2.7.0 in ./venv38/lib/python3.8/site-packages (from flake8==3.9.2->-r requirements.txt (line 18)) (2.7.0)\n","Requirement already satisfied: mccabe<0.7.0,>=0.6.0 in ./venv38/lib/python3.8/site-packages (from flake8==3.9.2->-r requirements.txt (line 18)) (0.6.1)\n","Requirement already satisfied: astroid<2.7,>=2.5.6 in ./venv38/lib/python3.8/site-packages (from pylint==2.8.2->-r requirements.txt (line 19)) (2.6.6)\n","Requirement already satisfied: isort<6,>=4.2.5 in ./venv38/lib/python3.8/site-packages (from pylint==2.8.2->-r requirements.txt (line 19)) (5.13.2)\n","Requirement already satisfied: typed-ast<1.5.0,>=1.4.0 in ./venv38/lib/python3.8/site-packages (from mypy==0.812->-r requirements.txt (line 20)) (1.4.3)\n","Requirement already satisfied: mypy-extensions<0.5.0,>=0.4.3 in ./venv38/lib/python3.8/site-packages (from mypy==0.812->-r requirements.txt (line 20)) (0.4.4)\n","Requirement already satisfied: certifi>=2017.4.17 in ./venv38/lib/python3.8/site-packages (from requests==2.27.1->-r requirements.txt (line 24)) (2023.11.17)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in ./venv38/lib/python3.8/site-packages (from requests==2.27.1->-r requirements.txt (line 24)) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in ./venv38/lib/python3.8/site-packages (from requests==2.27.1->-r requirements.txt (line 24)) (3.6)\n","Requirement already satisfied: lazy-object-proxy>=1.4.0 in ./venv38/lib/python3.8/site-packages (from astroid<2.7,>=2.5.6->pylint==2.8.2->-r requirements.txt (line 19)) (1.10.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in ./venv38/lib/python3.8/site-packages (from aiohttp->datasets==1.18.3->-r requirements.txt (line 1)) (6.0.4)\n","Requirement already satisfied: yarl<2.0,>=1.0 in ./venv38/lib/python3.8/site-packages (from aiohttp->datasets==1.18.3->-r requirements.txt (line 1)) (1.9.4)\n","Requirement already satisfied: frozenlist>=1.1.1 in ./venv38/lib/python3.8/site-packages (from aiohttp->datasets==1.18.3->-r requirements.txt (line 1)) (1.4.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in ./venv38/lib/python3.8/site-packages (from aiohttp->datasets==1.18.3->-r requirements.txt (line 1)) (1.3.1)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in ./venv38/lib/python3.8/site-packages (from aiohttp->datasets==1.18.3->-r requirements.txt (line 1)) (4.0.3)\n","Requirement already satisfied: MarkupSafe>=2.0 in ./venv38/lib/python3.8/site-packages (from jinja2<4.0,>=3.0->snakemake==7.1.1->-r requirements.txt (line 11)) (2.1.4)\n","Requirement already satisfied: dpath<3.0,>=2.1 in ./venv38/lib/python3.8/site-packages (from yte<2.0,>=1.0->snakemake==7.1.1->-r requirements.txt (line 11)) (2.1.6)\n","Requirement already satisfied: plac<2.0.0,>=1.3.4 in ./venv38/lib/python3.8/site-packages (from yte<2.0,>=1.0->snakemake==7.1.1->-r requirements.txt (line 11)) (1.4.2)\n","Requirement already satisfied: gitdb<5,>=4.0.1 in ./venv38/lib/python3.8/site-packages (from gitpython->snakemake==7.1.1->-r requirements.txt (line 11)) (4.0.11)\n","Requirement already satisfied: importlib-resources>=1.4.0 in ./venv38/lib/python3.8/site-packages (from jsonschema->snakemake==7.1.1->-r requirements.txt (line 11)) (6.1.1)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in ./venv38/lib/python3.8/site-packages (from jsonschema->snakemake==7.1.1->-r requirements.txt (line 11)) (2023.12.1)\n","Requirement already satisfied: pkgutil-resolve-name>=1.3.10 in ./venv38/lib/python3.8/site-packages (from jsonschema->snakemake==7.1.1->-r requirements.txt (line 11)) (1.3.10)\n","Requirement already satisfied: referencing>=0.28.4 in ./venv38/lib/python3.8/site-packages (from jsonschema->snakemake==7.1.1->-r requirements.txt (line 11)) (0.33.0)\n","Requirement already satisfied: rpds-py>=0.7.1 in ./venv38/lib/python3.8/site-packages (from jsonschema->snakemake==7.1.1->-r requirements.txt (line 11)) (0.17.1)\n","Requirement already satisfied: fastjsonschema in ./venv38/lib/python3.8/site-packages (from nbformat->snakemake==7.1.1->-r requirements.txt (line 11)) (2.19.1)\n","Requirement already satisfied: jupyter-core in ./venv38/lib/python3.8/site-packages (from nbformat->snakemake==7.1.1->-r requirements.txt (line 11)) (5.7.1)\n","Requirement already satisfied: traitlets>=5.1 in ./venv38/lib/python3.8/site-packages (from nbformat->snakemake==7.1.1->-r requirements.txt (line 11)) (5.14.1)\n","Requirement already satisfied: decorator>=3.4.2 in ./venv38/lib/python3.8/site-packages (from retry->snakemake==7.1.1->-r requirements.txt (line 11)) (5.1.1)\n","Requirement already satisfied: smmap<6,>=3.0.1 in ./venv38/lib/python3.8/site-packages (from gitdb<5,>=4.0.1->gitpython->snakemake==7.1.1->-r requirements.txt (line 11)) (5.0.1)\n","Requirement already satisfied: zipp>=3.1.0 in ./venv38/lib/python3.8/site-packages (from importlib-resources>=1.4.0->jsonschema->snakemake==7.1.1->-r requirements.txt (line 11)) (3.17.0)\n","Requirement already satisfied: platformdirs>=2.5 in ./venv38/lib/python3.8/site-packages (from jupyter-core->nbformat->snakemake==7.1.1->-r requirements.txt (line 11)) (4.1.0)\n","Installing collected packages: numpy\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 1.24.4\n","    Uninstalling numpy-1.24.4:\n","      Successfully uninstalled numpy-1.24.4\n","Successfully installed numpy-1.19.0\n","\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n","echo \"import nltk \\nnltk.download('punkt')\" | python3 /dev/stdin\n","[nltk_data] Downloading package punkt to /home/ken/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","pip install --upgrade numpy==1.23\n","Collecting numpy==1.23\n","  Downloading numpy-1.23.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hInstalling collected packages: numpy\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 1.19.0\n","    Uninstalling numpy-1.19.0:\n","      Successfully uninstalled numpy-1.19.0\n","Successfully installed numpy-1.23.0\n","\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"]}],"source":["! make setup_for_updating"]},{"cell_type":"markdown","metadata":{"id":"UleWUNAcqoL-"},"source":["If you need to download the model checkpoints for the best classifier and NER models, run the cell below. If the `train_and_predict` pipeline was run, then the models are already present."]},{"cell_type":"code","execution_count":4,"metadata":{"id":"RcQ3mQhiqoi-"},"outputs":[{"name":"stdout","output_type":"stream","text":["--2024-01-27 11:08:30--  https://huggingface.co/globalbiodata/inventory/resolve/main/article_classifier.pt\n","Resolving huggingface.co (huggingface.co)... 18.155.173.64, 18.155.173.45, 18.155.173.122, ...\n","Connecting to huggingface.co (huggingface.co)|18.155.173.64|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://cdn-lfs.huggingface.co/repos/0c/71/0c71d65c1ad08d6d90c7a439d78f4e3d7683b4b87ed90f6649fde934a1e6a69d/5718a7f70becacb46d46501734c83aab81c86feec563594f6a25c116aa31b521?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27article_classifier.pt%3B+filename%3D%22article_classifier.pt%22%3B&Expires=1706639144&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwNjYzOTE0NH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy8wYy83MS8wYzcxZDY1YzFhZDA4ZDZkOTBjN2E0MzlkNzhmNGUzZDc2ODNiNGI4N2VkOTBmNjY0OWZkZTkzNGExZTZhNjlkLzU3MThhN2Y3MGJlY2FjYjQ2ZDQ2NTAxNzM0YzgzYWFiODFjODZmZWVjNTYzNTk0ZjZhMjVjMTE2YWEzMWI1MjE%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=0iv4dr5%7EIJeZ%7E8n-bOxI7sa%7EsJq9MRX-aPQwQFJgt1uy8sjU8ydh-p6Yc4DK7d7K04TJ-stBtFNJpqhJnr%7E9Z5KBe04wHVwLfdmkdSL0uHBm6OmesnwKKFB2OY6XLUxFy0QfXsu08wN8Fw8uiy3oT0sq5BHhhckVRr2u6WZ2NNZ5zNr4aVA4JPxb7WPV7hMj1ZDHwdKjWCyOpwzzas%7EBl6%7EqkYQzr0-ZK5x76PXPqcDoDd3gWNNkeFo5Nk1sk6bi0iR3xtqU-zMmNiRxWarO8QmnYeqsLzd3tqL8I9wuNhF-n%7EXT9VSI67MS%7EtGzYeUYytgbfr7tKxw2oTfJHVn74Q__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n","--2024-01-27 11:08:30--  https://cdn-lfs.huggingface.co/repos/0c/71/0c71d65c1ad08d6d90c7a439d78f4e3d7683b4b87ed90f6649fde934a1e6a69d/5718a7f70becacb46d46501734c83aab81c86feec563594f6a25c116aa31b521?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27article_classifier.pt%3B+filename%3D%22article_classifier.pt%22%3B&Expires=1706639144&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwNjYzOTE0NH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy8wYy83MS8wYzcxZDY1YzFhZDA4ZDZkOTBjN2E0MzlkNzhmNGUzZDc2ODNiNGI4N2VkOTBmNjY0OWZkZTkzNGExZTZhNjlkLzU3MThhN2Y3MGJlY2FjYjQ2ZDQ2NTAxNzM0YzgzYWFiODFjODZmZWVjNTYzNTk0ZjZhMjVjMTE2YWEzMWI1MjE%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=0iv4dr5%7EIJeZ%7E8n-bOxI7sa%7EsJq9MRX-aPQwQFJgt1uy8sjU8ydh-p6Yc4DK7d7K04TJ-stBtFNJpqhJnr%7E9Z5KBe04wHVwLfdmkdSL0uHBm6OmesnwKKFB2OY6XLUxFy0QfXsu08wN8Fw8uiy3oT0sq5BHhhckVRr2u6WZ2NNZ5zNr4aVA4JPxb7WPV7hMj1ZDHwdKjWCyOpwzzas%7EBl6%7EqkYQzr0-ZK5x76PXPqcDoDd3gWNNkeFo5Nk1sk6bi0iR3xtqU-zMmNiRxWarO8QmnYeqsLzd3tqL8I9wuNhF-n%7EXT9VSI67MS%7EtGzYeUYytgbfr7tKxw2oTfJHVn74Q__&Key-Pair-Id=KVTP0A1DKRTAX\n","Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 18.155.173.8, 18.155.173.61, 18.155.173.127, ...\n","Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|18.155.173.8|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 498674413 (476M) [binary/octet-stream]\n","Saving to: ‘out/classif_train_out/article_classifier.pt’\n","\n","out/classif_train_o 100%[===================>] 475.57M  5.59MB/s    in 85s     \n","\n","2024-01-27 11:09:55 (5.61 MB/s) - ‘out/classif_train_out/article_classifier.pt’ saved [498674413/498674413]\n","\n","out/classif_train_out/article_classifier.pt: OK\n","--2024-01-27 11:09:58--  https://huggingface.co/globalbiodata/inventory/resolve/main/named_entity_recognition.pt\n","Resolving huggingface.co (huggingface.co)... 18.155.173.64, 18.155.173.126, 18.155.173.122, ...\n","Connecting to huggingface.co (huggingface.co)|18.155.173.64|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://cdn-lfs.huggingface.co/repos/0c/71/0c71d65c1ad08d6d90c7a439d78f4e3d7683b4b87ed90f6649fde934a1e6a69d/dc0bc8b4929e33da52bc92e12720260b392421883889e0a36c809cb0b5c40f5d?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27named_entity_recognition.pt%3B+filename%3D%22named_entity_recognition.pt%22%3B&Expires=1706639225&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwNjYzOTIyNX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy8wYy83MS8wYzcxZDY1YzFhZDA4ZDZkOTBjN2E0MzlkNzhmNGUzZDc2ODNiNGI4N2VkOTBmNjY0OWZkZTkzNGExZTZhNjlkL2RjMGJjOGI0OTI5ZTMzZGE1MmJjOTJlMTI3MjAyNjBiMzkyNDIxODgzODg5ZTBhMzZjODA5Y2IwYjVjNDBmNWQ%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=WCZTu8gsSmGA-R88jBDLIr4aoTQ0mstfqJs7damJ0OvldEAhOOCKysqkH4fGXAKDf72xwY11NNLrJjkubKYEiPAo5gHehpUT7KGMh21OlhX3O4qQdlOqFyJFgR%7E-5MePqsJ278xuOU1DM6l6CcfSkt1Cw9VCbeusu-XeGNu2D4AtrMOoxEzOwONFuQQUnbFoJ4iZjXRgAGhvYCQZZl-A34Txfs6Ge2W1NW3KE5tD1v7FzS3l0QOKX59Crq3727zBQdQR821IkElqmELRr63tjxFL0z2hSYya5aQQgn9fBZ3qIPHmhKDQFlnqPJlvXiTC6k35urvPUG%7EZJrmi3mLf4g__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n","--2024-01-27 11:09:58--  https://cdn-lfs.huggingface.co/repos/0c/71/0c71d65c1ad08d6d90c7a439d78f4e3d7683b4b87ed90f6649fde934a1e6a69d/dc0bc8b4929e33da52bc92e12720260b392421883889e0a36c809cb0b5c40f5d?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27named_entity_recognition.pt%3B+filename%3D%22named_entity_recognition.pt%22%3B&Expires=1706639225&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwNjYzOTIyNX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy8wYy83MS8wYzcxZDY1YzFhZDA4ZDZkOTBjN2E0MzlkNzhmNGUzZDc2ODNiNGI4N2VkOTBmNjY0OWZkZTkzNGExZTZhNjlkL2RjMGJjOGI0OTI5ZTMzZGE1MmJjOTJlMTI3MjAyNjBiMzkyNDIxODgzODg5ZTBhMzZjODA5Y2IwYjVjNDBmNWQ%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=WCZTu8gsSmGA-R88jBDLIr4aoTQ0mstfqJs7damJ0OvldEAhOOCKysqkH4fGXAKDf72xwY11NNLrJjkubKYEiPAo5gHehpUT7KGMh21OlhX3O4qQdlOqFyJFgR%7E-5MePqsJ278xuOU1DM6l6CcfSkt1Cw9VCbeusu-XeGNu2D4AtrMOoxEzOwONFuQQUnbFoJ4iZjXRgAGhvYCQZZl-A34Txfs6Ge2W1NW3KE5tD1v7FzS3l0QOKX59Crq3727zBQdQR821IkElqmELRr63tjxFL0z2hSYya5aQQgn9fBZ3qIPHmhKDQFlnqPJlvXiTC6k35urvPUG%7EZJrmi3mLf4g__&Key-Pair-Id=KVTP0A1DKRTAX\n","Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 18.155.173.61, 18.155.173.8, 18.155.173.127, ...\n","Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|18.155.173.61|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 496321009 (473M) [binary/octet-stream]\n","Saving to: ‘out/ner_train_out/named_entity_recognition.pt’\n","\n","out/ner_train_out/n 100%[===================>] 473.33M  6.73MB/s    in 79s     \n","\n","2024-01-27 11:11:18 (5.97 MB/s) - ‘out/ner_train_out/named_entity_recognition.pt’ saved [496321009/496321009]\n","\n","out/ner_train_out/named_entity_recognition.pt: OK\n"]}],"source":["# Get trained article classification model\n","# Create output directory\n","! mkdir -p out/classif_train_out/best\n","# Print name of model to the necessary file\n","! echo \"out/classif_train_out/article_classifier.pt\" > out/classif_train_out/best/best_checkpt.txt\n","# Download the model\n","! wget -O \"out/classif_train_out/article_classifier.pt\" https://huggingface.co/globalbiodata/inventory/resolve/main/article_classifier.pt\n","# Check that it downloaded properly\n","! echo \"5718a7f70becacb46d46501734c83aab81c86feec563594f6a25c116aa31b521 out/classif_train_out/article_classifier.pt\" | sha256sum -c\n","\n","# Get trained NER model\n","! mkdir -p out/ner_train_out/best\n","! echo \"out/ner_train_out/named_entity_recognition.pt\" > out/ner_train_out/best/best_checkpt.txt\n","! wget -O \"out/ner_train_out/named_entity_recognition.pt\" https://huggingface.co/globalbiodata/inventory/resolve/main/named_entity_recognition.pt\n","! echo \"dc0bc8b4929e33da52bc92e12720260b392421883889e0a36c809cb0b5c40f5d out/ner_train_out/named_entity_recognition.pt\" | sha256sum -c"]},{"cell_type":"markdown","metadata":{"id":"CFB8BHYk8AwS"},"source":["# Setting up Configurations\n","\n","Before running the automated pipelines, first update the configuration file `config/update_inventory.yml`. It can be accessed in Google Drive, though you may need to download it and edit it in a text editor such as Notepad, then reupload it.\n","\n","* **Europe PMC query publication date range**: These are stored as variables `query_from_date` and `query_to_date` in that file. Note that the dates are inclusive. For example to get papers published in 2022, both of those varibles should be 2022.\n","* **Previous inventory file**: During strict deduplication and flagging for manual review, the results of the previous inventory are taken into account. Specify the location of the most recent inventory output file in the variable `previous_inventory`."]},{"cell_type":"markdown","metadata":{"id":"XG8imhT0bms7"},"source":["# Running the pipeline\n","---\n","Now, we are ready to run the pipeline\n","\n","## Run it\n","\n","The following cell will run the pipeline described above. It may take a while, but GPU will speed it up a lot."]},{"cell_type":"code","execution_count":9,"metadata":{"id":"zFSmOvuUnSPE"},"outputs":[{"name":"stdout","output_type":"stream","text":["snakemake \\\n","-s snakemake/update_inventory.smk \\\n","--configfile config/update_inventory.yml -c1\n","\u001b[33mBuilding DAG of jobs...\u001b[0m\n","\u001b[33mUsing shell: /usr/bin/bash\u001b[0m\n","\u001b[33mProvided cores: 1 (use --cores to define parallelism)\u001b[0m\n","\u001b[33mRules claiming more threads will be scaled down.\u001b[0m\n","\u001b[33mJob stats:\n","job                      count    min threads    max threads\n","---------------------  -------  -------------  -------------\n","all                          1              1              1\n","classify_papers              1              1              1\n","extract_urls                 1              1              1\n","filter_positives             1              1              1\n","flag_for_review              1              1              1\n","initial_deduplication        1              1              1\n","ner_predict                  1              1              1\n","process_names                1              1              1\n","total                        8              1              1\n","\u001b[0m\n","\u001b[33mSelect jobs to execute...\u001b[0m\n","\u001b[32m\u001b[0m\n","\u001b[32m[Sat Jan 27 12:35:51 2024]\u001b[0m\n","\u001b[32mrule classify_papers:\n","    input: out/classif_train_out/best/best_checkpt.txt, out/new_query/query_results.csv\n","    output: out/new_query/classification/predictions.csv\n","    jobid: 7\n","    resources: tmpdir=/tmp\u001b[0m\n","\u001b[32m\u001b[0m\n","Some weights of the model checkpoint at allenai/dsp_roberta_base_dapt_biomed_tapt_rct_500 were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at allenai/dsp_roberta_base_dapt_biomed_tapt_rct_500 and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Parameter 'function'=<function tokenize_text.<locals>.<lambda> at 0x7f9d3df60160> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n","100%|█████████████████████████████████████████████| 3/3 [00:00<00:00,  4.20ba/s]\n","Done. Saved predictions to out/new_query/classification/predictions.csv\n","\u001b[32m[Sat Jan 27 12:57:11 2024]\u001b[0m\n","\u001b[32mFinished job 7.\u001b[0m\n","\u001b[32m1 of 8 steps (12%) done\u001b[0m\n","\u001b[33mSelect jobs to execute...\u001b[0m\n","\u001b[32m\u001b[0m\n","\u001b[32m[Sat Jan 27 12:57:11 2024]\u001b[0m\n","\u001b[32mrule filter_positives:\n","    input: out/new_query/classification/predictions.csv\n","    output: out/new_query/classification/predicted_positives.csv\n","    jobid: 6\n","    resources: tmpdir=/tmp\u001b[0m\n","\u001b[32m\u001b[0m\n","\u001b[32m[Sat Jan 27 12:57:11 2024]\u001b[0m\n","\u001b[32mFinished job 6.\u001b[0m\n","\u001b[32m2 of 8 steps (25%) done\u001b[0m\n","\u001b[33mSelect jobs to execute...\u001b[0m\n","\u001b[32m\u001b[0m\n","\u001b[32m[Sat Jan 27 12:57:11 2024]\u001b[0m\n","\u001b[32mrule ner_predict:\n","    input: out/new_query/classification/predicted_positives.csv, out/ner_train_out/best/best_checkpt.txt\n","    output: out/new_query/ner/predictions.csv\n","    jobid: 5\n","    resources: tmpdir=/tmp\u001b[0m\n","\u001b[32m\u001b[0m\n","Some weights of the model checkpoint at allenai/dsp_roberta_base_dapt_biomed_tapt_rct_500 were not used when initializing RobertaForTokenClassification: ['lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n","- This IS expected if you are initializing RobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at allenai/dsp_roberta_base_dapt_biomed_tapt_rct_500 and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Done. Saved predictions to out/new_query/ner/predictions.csv.\n","\u001b[32m[Sat Jan 27 13:03:35 2024]\u001b[0m\n","\u001b[32mFinished job 5.\u001b[0m\n","\u001b[32m3 of 8 steps (38%) done\u001b[0m\n","\u001b[33mSelect jobs to execute...\u001b[0m\n","\u001b[32m\u001b[0m\n","\u001b[32m[Sat Jan 27 13:03:35 2024]\u001b[0m\n","\u001b[32mrule extract_urls:\n","    input: out/new_query/ner/predictions.csv\n","    output: out/new_query/url_extraction/predictions.csv\n","    jobid: 4\n","    resources: tmpdir=/tmp\u001b[0m\n","\u001b[32m\u001b[0m\n","Done. Wrote output to out/new_query/url_extraction/predictions.csv.\n","\u001b[32m[Sat Jan 27 13:03:36 2024]\u001b[0m\n","\u001b[32mFinished job 4.\u001b[0m\n","\u001b[32m4 of 8 steps (50%) done\u001b[0m\n","\u001b[33mSelect jobs to execute...\u001b[0m\n","\u001b[32m\u001b[0m\n","\u001b[32m[Sat Jan 27 13:03:36 2024]\u001b[0m\n","\u001b[32mrule process_names:\n","    input: out/new_query/url_extraction/predictions.csv\n","    output: out/new_query/processed_names/predictions.csv\n","    jobid: 3\n","    resources: tmpdir=/tmp\u001b[0m\n","\u001b[32m\u001b[0m\n","Done processing names.\n","0 articles with no names removed.\n","Wrote output to out/new_query/processed_names/predictions.csv.\n","\u001b[32m[Sat Jan 27 13:03:38 2024]\u001b[0m\n","\u001b[32mFinished job 3.\u001b[0m\n","\u001b[32m5 of 8 steps (62%) done\u001b[0m\n","\u001b[33mSelect jobs to execute...\u001b[0m\n","\u001b[32m\u001b[0m\n","\u001b[32m[Sat Jan 27 13:03:38 2024]\u001b[0m\n","\u001b[32mrule initial_deduplication:\n","    input: out/new_query/processed_names/predictions.csv, data/final_inventory_2022.csv\n","    output: out/new_query/initial_deduplication/predictions.csv\n","    jobid: 2\n","    resources: tmpdir=/tmp\u001b[0m\n","\u001b[32m\u001b[0m\n","Done. Wrote output to out/new_query/initial_deduplication/predictions.csv.\n","\u001b[32m[Sat Jan 27 13:03:39 2024]\u001b[0m\n","\u001b[32mFinished job 2.\u001b[0m\n","\u001b[32m6 of 8 steps (75%) done\u001b[0m\n","\u001b[33mSelect jobs to execute...\u001b[0m\n","\u001b[32m\u001b[0m\n","\u001b[32m[Sat Jan 27 13:03:39 2024]\u001b[0m\n","\u001b[32mrule flag_for_review:\n","    input: out/new_query/initial_deduplication/predictions.csv\n","    output: out/new_query/for_manual_review/predictions.csv\n","    jobid: 1\n","    resources: tmpdir=/tmp\u001b[0m\n","\u001b[32m\u001b[0m\n","Total number of flagged rows: 982\n","Rows with duplicate names: 105\n","Rows with duplicate URLs: 50\n","Rows with low probability name: 857\n","Wrote output to out/new_query/for_manual_review/predictions.csv.\n","Inventory flagged for manual review.\n","Once manual review is finished place file in out/new_query/manually_reviewed.\n","\u001b[32m[Sat Jan 27 13:03:45 2024]\u001b[0m\n","\u001b[32mFinished job 1.\u001b[0m\n","\u001b[32m7 of 8 steps (88%) done\u001b[0m\n","\u001b[33mSelect jobs to execute...\u001b[0m\n","\u001b[32m\u001b[0m\n","\u001b[32m[Sat Jan 27 13:03:45 2024]\u001b[0m\n","\u001b[32mlocalrule all:\n","    input: out/new_query/for_manual_review/predictions.csv\n","    jobid: 0\n","    resources: tmpdir=/tmp\u001b[0m\n","\u001b[32m\u001b[0m\n","\u001b[32m[Sat Jan 27 13:03:45 2024]\u001b[0m\n","\u001b[32mFinished job 0.\u001b[0m\n","\u001b[32m8 of 8 steps (100%) done\u001b[0m\n","\u001b[33mComplete log: /home/ken/documents/work/gbc/inventory_2022/.snakemake/log/2024-01-27T123550.977814.snakemake.log\u001b[0m\n"]}],"source":["! make update_inventory"]},{"cell_type":"markdown","metadata":{"id":"mdwb9NveMdP0"},"source":["# Selective Manual Review\n","\n","After running the initial pipeline, the inventory has been flagged for selective manual review.\n","\n","The file to be reviewed is located at:\n","\n","`out/new_query/for_manual_review/predictions.csv`\n","\n","Review the flagged columns according to the instruction sheet ([doi: 10.5281/zenodo.7768363](https://doi.org/10.5281/zenodo.7768363)), then place the manually reviewed file in the following folder:\n","\n","`out/new_query/manually_reviewed/`\n","\n","The file must still be named `predictions.csv`\n","\n","# Processing Manual Review\n","\n","Next, further processing is performed on the manually reviewed inventory."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":42670,"status":"ok","timestamp":1675109651846,"user":{"displayName":"Kenneth Schackart","userId":"14619721059788161882"},"user_tz":420},"id":"z_c4ZbgONIoo","outputId":"5c69eb01-f62f-4df6-cd20-cd7512d0bba6"},"outputs":[{"name":"stdout","output_type":"stream","text":["snakemake \\\n","-s snakemake/update_inventory.smk \\\n","--configfile config/update_inventory.yml \\\n","-c 1 \\\n","--until process_countries\n","\u001b[33mBuilding DAG of jobs...\u001b[0m\n","\u001b[33mUsing shell: /usr/bin/bash\u001b[0m\n","\u001b[33mProvided cores: 1 (use --cores to define parallelism)\u001b[0m\n","\u001b[33mRules claiming more threads will be scaled down.\u001b[0m\n","\u001b[33mJob stats:\n","job                  count    min threads    max threads\n","-----------------  -------  -------------  -------------\n","check_urls               1              1              1\n","get_epmc_meta            1              1              1\n","process_countries        1              1              1\n","total                    3              1              1\n","\u001b[0m\n","\u001b[33mSelect jobs to execute...\u001b[0m\n","\u001b[32m\u001b[0m\n","\u001b[32m[Mon Jan 30 20:13:29 2023]\u001b[0m\n","\u001b[32mrule check_urls:\n","    input: out/new_query/processed_manual_review/predictions.csv\n","    output: out/new_query/url_checking/predictions.csv\n","    jobid: 11\n","    reason: Missing output files: out/new_query/url_checking/predictions.csv\n","    resources: tmpdir=/tmp\u001b[0m\n","\u001b[32m\u001b[0m\n","/usr/local/lib/python3.8/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.1\n","  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","Done. Wrote output to out/new_query/url_checking/predictions.csv.\n","\u001b[32m[Mon Jan 30 20:13:59 2023]\u001b[0m\n","\u001b[32mFinished job 11.\u001b[0m\n","\u001b[32m1 of 3 steps (33%) done\u001b[0m\n","\u001b[33mSelect jobs to execute...\u001b[0m\n","\u001b[32m\u001b[0m\n","\u001b[32m[Mon Jan 30 20:13:59 2023]\u001b[0m\n","\u001b[32mrule get_epmc_meta:\n","    input: out/new_query/url_checking/predictions.csv\n","    output: out/new_query/epmc_meta/predictions.csv\n","    jobid: 10\n","    reason: Missing output files: out/new_query/epmc_meta/predictions.csv; Input files updated by another job: out/new_query/url_checking/predictions.csv\n","    resources: tmpdir=/tmp\u001b[0m\n","\u001b[32m\u001b[0m\n","/usr/local/lib/python3.8/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.1\n","  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","Done. Wrote output to out/new_query/epmc_meta/predictions.csv.\n","\u001b[32m[Mon Jan 30 20:14:06 2023]\u001b[0m\n","\u001b[32mFinished job 10.\u001b[0m\n","\u001b[32m2 of 3 steps (67%) done\u001b[0m\n","\u001b[33mSelect jobs to execute...\u001b[0m\n","\u001b[32m\u001b[0m\n","\u001b[32m[Mon Jan 30 20:14:06 2023]\u001b[0m\n","\u001b[32mrule process_countries:\n","    input: out/new_query/epmc_meta/predictions.csv\n","    output: out/new_query/processed_countries/predictions.csv\n","    jobid: 9\n","    reason: Missing output files: out/new_query/processed_countries/predictions.csv; Input files updated by another job: out/new_query/epmc_meta/predictions.csv\n","    resources: tmpdir=/tmp\u001b[0m\n","\u001b[32m\u001b[0m\n","/usr/local/lib/python3.8/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.1\n","  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","Done. Wrote output to out/new_query/processed_countries/predictions.csv.\n","\u001b[32m[Mon Jan 30 20:14:10 2023]\u001b[0m\n","\u001b[32mFinished job 9.\u001b[0m\n","\u001b[32m3 of 3 steps (100%) done\u001b[0m\n","\u001b[33mComplete log: .snakemake/log/2023-01-30T201328.761519.snakemake.log\u001b[0m\n"]}],"source":["! make process_manually_reviewed_update"]},{"cell_type":"markdown","metadata":{"id":"AV4p2VA_NUfi"},"source":["## Final inventory\n","\n","The final inventory, including names, URLS, and metadata is found in the file:\n","*    `out/new_query/processed_countries/predictions.csv`"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.18"}},"nbformat":4,"nbformat_minor":0}
